{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Linguistic Analysis using NLP\n",
    "*Author: Aleksander Moeslund Wael*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages needed for the assignment.\n",
    "import os # For navigating the file system.\n",
    "import spacy # For NLP.\n",
    "import pandas as pd # For data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing to do is to load the spacy model. Only need to do this once.\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# I set a single path to the folder with the corpus.\n",
    "path_to_folder = os.path.join(os.getcwd(), \"in\", \"USEcorpus\")\n",
    "\n",
    "# Since there are 14 subfolders, I need to loop over them. I use os.listdir to get a list of the subfolders.\n",
    "subfolders = os.listdir(path_to_folder)\n",
    "\n",
    "\n",
    "# THIS IS THE COMPLETE LOOP WHICH RETURNS A .CSV FILE FOR EACH SUBFOLDER\n",
    "# There are 3 loops nested in total. The first loop is over the subfolders.\n",
    "for x in subfolders:\n",
    "    \n",
    "    # I join the folder path with the subfolder name to get the full path to the subfolder.\n",
    "    folder = os.path.join(path_to_folder, x)\n",
    "    \n",
    "    # I create an empty dataframe here, because I want a dataframe for each subfolder.\n",
    "    df = pd.DataFrame(columns=[\"text_name\", \"RelFreq_NOUN\", \"RelFreq_VERB\", \"RelFreq_ADJ\", \"RelFreq_ADV\", \"Unique_PER\", \"Unique_LOC\", \"Unique_ORG\"])\n",
    "\n",
    "    # The second loop is over the files in the subfolder.\n",
    "    for y in os.listdir(folder): \n",
    "        \n",
    "        # Defining the full path to the file\n",
    "        file_path = os.path.join(folder, y) # NEED FULL PATH\n",
    "        \n",
    "        # Reading the file\n",
    "        f = open(file_path, \"r\", encoding=\"ISO-8859-1\")\n",
    "        file = f.read()\n",
    "        \n",
    "        # I now create a spacy object by passing the text to the NLP object.\n",
    "        file_text = nlp(file)\n",
    "        \n",
    "        # Here is where I create lists for the POS and NER tags.\n",
    "        # The non_words_counter is used to remove punctutation, spaces, symbols and other non-words from the relative frequency calculations.\n",
    "        non_words_counter = 0\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        adjectives = []\n",
    "        adverbs = []\n",
    "        PER = []\n",
    "        LOC = []\n",
    "        ORG = []\n",
    "\n",
    "        # The third loop is over the tokens in the text.\n",
    "        for token in file_text:\n",
    "            \n",
    "            # Here, I use a series of if statements to check for the POS and NER tags and append them to the appropriate list.\n",
    "            # I use if and not elif statements, because tokens can have both a POS and a NER tag.\n",
    "            if token.pos_ == \"X\" or token.pos == \"PUNCT\" or token.pos == \"SPACE\" or token.pos == \"SYM\":\n",
    "                non_words_counter += 1\n",
    "            if token.pos_ == \"NOUN\":\n",
    "                nouns.append(token.text)\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verbs.append(token.text)\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.text)\n",
    "            if token.pos_ == \"ADV\":\n",
    "                adverbs.append(token.text)\n",
    "            if token.ent_type_ == \"PER\":\n",
    "                PER.append(token.text)\n",
    "            if token.ent_type_ == \"LOC\":\n",
    "                LOC.append(token.text)\n",
    "            if token.ent_type_ == \"ORG\":\n",
    "                ORG.append(token.text)\n",
    "        \n",
    "        # Here, I calculate the relative frequencies and the amount of unique entities. I round the relative frequencies to 2 decimals.\n",
    "        RelFreq_NOUN = round(len(nouns) / (len(file_text) - non_words_counter) * 10000, 2)\n",
    "        RelFreq_VERB = round(len(verbs) / (len(file_text) - non_words_counter) * 10000, 2)\n",
    "        RelFreq_ADJ = round(len(adjectives) / (len(file_text) - non_words_counter) * 10000, 2)\n",
    "        RelFreq_ADV = round(len(adverbs) / (len(file_text) - non_words_counter) * 10000, 2)\n",
    "        Unique_PER = len(set(PER))\n",
    "        Unique_LOC = len(set(LOC))\n",
    "        Unique_ORG = len(set(ORG))\n",
    "        \n",
    "        # I then append the results to the dataframe, so each row in the df is a text file.\n",
    "        df = df.append({\"text_name\": y, \"RelFreq_NOUN\": RelFreq_NOUN, \"RelFreq_VERB\": RelFreq_VERB, \"RelFreq_ADJ\": RelFreq_ADJ, \"RelFreq_ADV\": RelFreq_ADV, \"Unique_PER\": Unique_PER, \"Unique_LOC\": Unique_LOC, \"Unique_ORG\": Unique_ORG}, ignore_index=True)\n",
    "    \n",
    "    # Lastly, as part of the first loop, I save the dataframe as a .csv file in the out folder. It gets named after the subfolder.\n",
    "    df.to_csv(os.path.join(os.getcwd(), \"out\", x + \".csv\"))\n",
    "    \n",
    "    # The result is a .csv file for each subfolder in the corpus located in the \"out\" folder.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60b11ee0da9165ffe4d57b51d8ddf757cceecf36156ec87366fb718e74194e05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
